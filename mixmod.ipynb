{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of potential `barber` methods\n",
    "\n",
    "The Barber should use some method to define bin definition criteria from a set of galaxy photometry without knowing the galaxy redshifts.\n",
    "The Barber will then be judged by the performance of those bin definitions in the space of true redshifts of those galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import warnings\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n",
    "\n",
    "We'll start with code from [tomo\\_challenge](https://github.com/LSSTDESC/tomo_challenge/blob/master/tomo_challenge/data.py) (copied for now, until it becomes installable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============\n",
    "# # Generate datasets. We choose the size big enough to see the scalability\n",
    "# # of the algorithms, but not too big to avoid too long running times\n",
    "# # ============\n",
    "# n_samples = 1500\n",
    "# noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "#                                       noise=.05)\n",
    "# noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "# blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "# no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# # Anisotropicly distributed data\n",
    "# random_state = 170\n",
    "# X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "# transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "# X_aniso = np.dot(X, transformation)\n",
    "# aniso = (X_aniso, y)\n",
    "\n",
    "# # blobs with varied variances\n",
    "# varied = datasets.make_blobs(n_samples=n_samples,\n",
    "#                              cluster_std=[1.0, 2.5, 0.5],\n",
    "#                              random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "nersc_path = '/global/projecta/projectdirs/lsst/groups/WL/users/zuntz/tomo_challenge_data/'\n",
    "url_root =  'https://portal.nersc.gov/project/lsst/txpipe/tomo_challenge_data/'\n",
    "# This is not supposed to be needed - I don't understand why in my shifter env the warning\n",
    "# is being repeated.\n",
    "warned = False\n",
    "\n",
    "def load_magnitudes_and_colors(filename, bands):\n",
    "    \"\"\"Load magnitudes, and compute colors from them,\n",
    "    from a training or validation file.\n",
    "\n",
    "    Note that there are other columns available in\n",
    "    the files that this function does not load, but are\n",
    "    available for your methods (mag errors, size, s/n).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        The name of the file to read, e.g. riz/training.hdf5\n",
    "\n",
    "    bands: str\n",
    "        The list of bands to read from the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: array\n",
    "        Dimension is nfeature x nrow, where nfeature = nband + ncolor\n",
    "        and ncolor = nband * (nband - 1) / 2\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the data file\n",
    "    f = h5py.File(filename)\n",
    "\n",
    "    # Get the number of features (mags + colors)\n",
    "    # and data points\n",
    "    ndata = f['ra'].size\n",
    "    nband = len(bands)\n",
    "    ncolor = (nband * (nband - 1)) // 2\n",
    "    nfeature = nband + ncolor\n",
    "\n",
    "    # np.empty is like np.zeros except it doesn't\n",
    "    # bother filling in the data with zeros, just\n",
    "    # allocates space.  We can use it because we\n",
    "    # are filling it in in a moment.  This gets\n",
    "    # transposed before we return it to match\n",
    "    # what sklearn expects\n",
    "    data = np.empty((nfeature, ndata))\n",
    "\n",
    "    # Read the magnitudes into the array\n",
    "    for i, b in enumerate(bands):\n",
    "        data[i] = f['mcal_mag_{}'.format(b)][:]\n",
    "\n",
    "    f.close()\n",
    "    global warned\n",
    "    if not warned:\n",
    "        warnings.warn(\"Setting inf (undetected) bands to mag=30\")\n",
    "        warned = True\n",
    "    data[:nband][~np.isfinite(data[:nband])] = 30.0\n",
    "\n",
    "    # Starting column for the colors\n",
    "    n = nband\n",
    "\n",
    "    # also get colors as data, from all the\n",
    "    # (non-symmetric) pairs.  Note that we are getting some\n",
    "    # redundant colors here, and some incorrect colors based\n",
    "    # on the choice to set undetected magnitudes to 30.\n",
    "    for i in range(nband):\n",
    "        for j in range(i+1, nband):\n",
    "            data[n] = data[i] - data[j]\n",
    "            n += 1\n",
    "    \n",
    "    # Return the data. sklearn wants it the other way around\n",
    "    # because data scientists are weird and think of data as\n",
    "    # lots of rows instead of lots of columns.\n",
    "    return data.T\n",
    "\n",
    "def load_redshift(filename):\n",
    "    \"\"\"Load a redshift column from a training or validation file\"\"\"\n",
    "    f = h5py.File(filename)\n",
    "    z = f['redshift_true'][:]\n",
    "    f.close()\n",
    "    return z    \n",
    "\n",
    "# take a subsample of size from sklearn example for now\n",
    "X = load_magnitudes_and_colors('/global/projecta/projectdirs/lsst/groups/WL/users/zuntz/tomo_challenge_data/riz/training.hdf5', 'riz')[:1500, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do something like [this](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py), exploring methods to make bin definitions in the space of photometry.\n",
    "We're starting with a simple GMM with a pre-specified number of bins.\n",
    "\n",
    "TODO:\n",
    "- more methods (see sklearn demo)\n",
    "- colors instead of (in addition to?) magnitudes\n",
    "- incorporate errors\n",
    "- use probability over space to define bin definitions in color/mag space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']), int(max(y_pred) + 1))))\n",
    "colors = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "nbins = len(colors)\n",
    "# add black color for outliers (if any)\n",
    "# colors = np.append(colors, [\"#000000\"])\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=nbins, covariance_type='full')\n",
    "Y = gmm.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytriangle(X, Y, colors):\n",
    "    \n",
    "    ngal, ndim = np.shape(X)\n",
    "    nrow = ndim - 1\n",
    "    cols, counts = np.unique(Y, return_counts=True)\n",
    "    ncol = len(cols)\n",
    "    \n",
    "    fig = plt.figure(figsize=(nrow*(5), nrow*5))\n",
    "    ax = [[fig.add_subplot(ndim, ndim, ndim * i + j + 1) for j in range(ndim)] for i in range(ndim)]\n",
    "#     to_keep = range(nrow)#[0, 1, 2, 4]\n",
    "    \n",
    "    for k in range(ncol):\n",
    "        Xplot = X[Y == k]\n",
    "        for i in range(ndim):\n",
    "            for j in range(i):\n",
    "                ax[i][j].scatter(Xplot[:, i], Xplot[:, j], s=1, alpha=1./np.log(counts[k]), color=colors[k])\n",
    "                ax[i][j].set_xlabel(str(i))\n",
    "                ax[i][j].set_ylabel(str(j))\n",
    "    plt.subplots_adjust(hspace=0., wspace=0.)\n",
    "#     plt.savefig('mixmod.png', dpi=100, bbox_inches='tight', pad_inches = 0)\n",
    "    return\n",
    "mytriangle(X, Y, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============\n",
    "# # Set up cluster parameters\n",
    "# # ============\n",
    "# plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "# plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "#                     hspace=.01)\n",
    "\n",
    "# plot_num = 1\n",
    "\n",
    "# default_base = {'quantile': .3,\n",
    "#                 'eps': .3,\n",
    "#                 'damping': .9,\n",
    "#                 'preference': -200,\n",
    "#                 'n_neighbors': 10,\n",
    "#                 'n_clusters': 3,\n",
    "#                 'min_samples': 20,\n",
    "#                 'xi': 0.05,\n",
    "#                 'min_cluster_size': 0.1}\n",
    "\n",
    "# datasets = [\n",
    "#     (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "#                      'quantile': .2, 'n_clusters': 2,\n",
    "#                      'min_samples': 20, 'xi': 0.25}),\n",
    "#     (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "#     (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "#               'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "#     (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "#              'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "#     (blobs, {}),\n",
    "#     (no_structure, {})]\n",
    "\n",
    "# for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "#     # update parameters with dataset-specific values\n",
    "#     params = default_base.copy()\n",
    "#     params.update(algo_params)\n",
    "\n",
    "#     X, y = dataset\n",
    "\n",
    "#     # normalize dataset for easier parameter selection\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "\n",
    "#     # estimate bandwidth for mean shift\n",
    "#     bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "#     # connectivity matrix for structured Ward\n",
    "#     connectivity = kneighbors_graph(\n",
    "#         X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "#     # make connectivity symmetric\n",
    "#     connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "#     # ============\n",
    "#     # Create cluster objects\n",
    "#     # ============\n",
    "#     ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "#     two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "#     ward = cluster.AgglomerativeClustering(\n",
    "#         n_clusters=params['n_clusters'], linkage='ward',\n",
    "#         connectivity=connectivity)\n",
    "#     spectral = cluster.SpectralClustering(\n",
    "#         n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "#         affinity=\"nearest_neighbors\")\n",
    "#     dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "#     optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
    "#                             xi=params['xi'],\n",
    "#                             min_cluster_size=params['min_cluster_size'])\n",
    "#     affinity_propagation = cluster.AffinityPropagation(\n",
    "#         damping=params['damping'], preference=params['preference'])\n",
    "#     average_linkage = cluster.AgglomerativeClustering(\n",
    "#         linkage=\"average\", affinity=\"cityblock\",\n",
    "#         n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "#     birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "#     gmm = mixture.GaussianMixture(\n",
    "#         n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "#     clustering_algorithms = (\n",
    "#         ('MiniBatchKMeans', two_means),\n",
    "#         ('AffinityPropagation', affinity_propagation),\n",
    "#         ('MeanShift', ms),\n",
    "#         ('SpectralClustering', spectral),\n",
    "#         ('Ward', ward),\n",
    "#         ('AgglomerativeClustering', average_linkage),\n",
    "#         ('DBSCAN', dbscan),\n",
    "#         ('OPTICS', optics),\n",
    "#         ('Birch', birch),\n",
    "#         ('GaussianMixture', gmm)\n",
    "#     )\n",
    "\n",
    "#     for name, algorithm in clustering_algorithms:\n",
    "#         t0 = time.time()\n",
    "\n",
    "#         # catch warnings related to kneighbors_graph\n",
    "#         with warnings.catch_warnings():\n",
    "#             warnings.filterwarnings(\n",
    "#                 \"ignore\",\n",
    "#                 message=\"the number of connected components of the \" +\n",
    "#                 \"connectivity matrix is [0-9]{1,2}\" +\n",
    "#                 \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "#                 category=UserWarning)\n",
    "#             warnings.filterwarnings(\n",
    "#                 \"ignore\",\n",
    "#                 message=\"Graph is not fully connected, spectral embedding\" +\n",
    "#                 \" may not work as expected.\",\n",
    "#                 category=UserWarning)\n",
    "#             algorithm.fit(X)\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         if hasattr(algorithm, 'labels_'):\n",
    "#             y_pred = algorithm.labels_.astype(np.int)\n",
    "#         else:\n",
    "#             y_pred = algorithm.predict(X)\n",
    "\n",
    "#         plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "#         if i_dataset == 0:\n",
    "#             plt.title(name, size=18)\n",
    "\n",
    "#         colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "#                                              '#f781bf', '#a65628', '#984ea3',\n",
    "#                                              '#999999', '#e41a1c', '#dede00']),\n",
    "#                                       int(max(y_pred) + 1))))\n",
    "#         # add black color for outliers (if any)\n",
    "#         colors = np.append(colors, [\"#000000\"])\n",
    "#         plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "#         plt.xlim(-2.5, 2.5)\n",
    "#         plt.ylim(-2.5, 2.5)\n",
    "#         plt.xticks(())\n",
    "#         plt.yticks(())\n",
    "#         plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "#                  transform=plt.gca().transAxes, size=15,\n",
    "#                  horizontalalignment='right')\n",
    "#         plot_num += 1\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
